{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **Creating a Simple Pipeline**\n",
    "\n",
    "A typical pipeline consists of:\n",
    "\n",
    "-   **Data preprocessing steps**: like scaling or imputation.\n",
    "-   **Modeling step**: like classification or regression.\n",
    "\n",
    "For this example, let's create a pipeline that:\n",
    "\n",
    "1.  Imputes missing values in the dataset.\n",
    "2.  Scales numerical features.\n",
    "3.  Uses a `RandomForestClassifier` as the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with missing values\n",
    "data = {\n",
    "    'age': [25, 30, 35, 40, 45, None, 50, 55],\n",
    "    'salary': [50000, 60000, 70000, 80000, 90000, 100000, None, 120000],\n",
    "    'target': [1, 0, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features (X) and target (y)\n",
    "X = df[['age', 'salary']]\n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Pipeline\n",
    "\n",
    "Here we will use the SimpleImputer to handle missing data, StandardScaler to scale the numerical data, and a RandomForestClassifier for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# pipeline for preprocessing and classification\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "                ('scaler', StandardScaler())  # Standardize the numerical features\n",
    "            ]), ['age', 'salary'])\n",
    "        ]\n",
    "    )),\n",
    "    ('classifier', RandomForestClassifier())  # Classifier step\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearchCV with Pipelines\n",
    "\n",
    "You can also perform hyperparameter tuning on the pipeline using GridSearchCV. This allows you to search through multiple combinations of hyperparameters for the preprocessing steps and the model.\n",
    "\n",
    "Grid Search Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kibe/projects/ml/data-science-and-ml-exercises/scikit-learn/.venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__n_estimators': 50, 'preprocessor__num__imputer__strategy': 'mean'}\n",
      "Best model score: 0.3333333333333333\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters for GridSearchCV\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],  # Imputation strategy\n",
    "    'classifier__n_estimators': [50, 100, 150]  # Number of trees for RandomForestClassifier\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best model performance\n",
    "print(\"Best model score:\", grid_search.best_score_)\n",
    "\n",
    "# Predictions with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Advantages of Using Pipelines**\n",
    "\n",
    "-   **Efficiency**: Pipelines allow you to automate preprocessing and modeling, reducing the chances of errors during data transformation or modeling.\n",
    "-   **Reproducibility**: Pipelines ensure that the same steps are applied to both the training and testing data, preventing data leakage and making the workflow easier to reproduce.\n",
    "-   **Ease of Deployment**: Once a pipeline is defined and tested, it can be deployed as a single unit, making the deployment process cleaner and more manageable.\n",
    "\n",
    "#####  **Advanced Pipeline Usage**\n",
    "\n",
    "You can also create more advanced pipelines with multiple preprocessing steps and different models, such as:\n",
    "\n",
    "-   **Feature selection** (e.g., using `SelectKBest`).\n",
    "-   **Custom transformers** for specific preprocessing needs.\n",
    "-   **Handling different feature types** (e.g., numerical and categorical features).\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('preprocessor', ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num', Pipeline([\n",
    "#                 ('imputer', SimpleImputer(strategy='mean')),\n",
    "#                 ('scaler', StandardScaler())\n",
    "#             ]), ['age', 'salary']),\n",
    "#             ('cat', OneHotEncoder(handle_unknown='ignore'), ['category'])  # handle unknown categories\n",
    "#         ]\n",
    "#     )),\n",
    "#     ('feature_selection', SelectKBest(k=2)),  # Select top 2 features\n",
    "#     ('classifier', RandomForestClassifier())\n",
    "# ])\n",
    "\n",
    "# # Fit and predict using the updated pipeline\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# predictions = pipeline.predict(X_test)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'salary'], dtype='object')\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Pipeline with feature selection\n",
    "print(X_train.columns)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), ['age', 'salary'])  # Standardize 'age' and 'salary'\n",
    "        ]\n",
    "    )),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit and predict using the updated pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "Pipeline in Scikit-learn helps streamline machine learning workflows by combining preprocessing and modeling steps into one object.\n",
    "\n",
    "You can use GridSearchCV for hyperparameter tuning within a pipeline.\n",
    "\n",
    "Pipelines make your workflow more reproducible, manageable, and less error-prone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
