{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. What is Boosting?**\n",
    "\n",
    "-   **Boosting** works by training multiple models sequentially. Each model corrects the errors made by the previous one, making the final prediction stronger and more accurate.\n",
    "-   Unlike **bagging** (which trains models independently), boosting adjusts the weight of misclassified data points, giving more importance to harder examples during training.\n",
    "\n",
    "The two main types of boosting algorithms are:\n",
    "\n",
    "-   **AdaBoost** (Adaptive Boosting)\n",
    "-   **Gradient Boosting**\n",
    "\n",
    "----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. AdaBoost**\n",
    "\n",
    "AdaBoost adjusts the weights of weak learners by focusing more on the misclassified instances, and it combines the results of all learners to make the final prediction. It works with any base classifier, but typically decision trees (also called \"stumps\") are used as weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. AdaBoost Example\n",
    "In this example, we will use AdaBoostClassifier with a decision tree stump (a very shallow decision tree) as the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kibe/projects/ml/data-science-and-ml-exercises/scikit-learn/.venv/lib/python3.10/site-packages/numpy/_core/getlimits.py:548: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Load Data\n",
    "We will again use the Iris dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target (species)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Initialize and Train AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kibe/projects/ml/data-science-and-ml-exercises/scikit-learn/.venv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize base classifier (decision tree stump)\n",
    "base_model = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Initialize AdaBoost classifier with 50 estimators\n",
    "adaboost_model = AdaBoostClassifier(base_model, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the AdaBoost model\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of AdaBoost: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Gradient Boosting\n",
    "Gradient Boosting builds models sequentially by fitting a new model to the residual errors of the previous model. It minimizes the error by fitting the models to the gradient of the error, hence the name \"gradient boosting.\"\n",
    "\n",
    "GradientBoostingClassifier is a more flexible algorithm compared to AdaBoost and typically performs better on structured data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Gradient Boosting Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gradient Boosting: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize GradientBoostingClassifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Accuracy of Gradient Boosting: {accuracy_gb:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Comparison Between AdaBoost and Gradient Boosting**\n",
    "\n",
    "-   **AdaBoost** adjusts the weights of the misclassified examples after each iteration, focusing on harder examples, while **Gradient Boosting** minimizes residual errors directly through gradient descent.\n",
    "-   **AdaBoost** generally performs well when you have a lot of noisy data and when the weak learners (base models) are very simple.\n",
    "-   **Gradient Boosting** is more powerful and can handle both regression and classification tasks with great flexibility, especially when the data is complex and has a lot of interactions.\n",
    "\n",
    "----------\n",
    "\n",
    "### **5. Pros and Cons of Boosting**\n",
    "\n",
    "#### **Pros:**\n",
    "\n",
    "-   **High Accuracy**: Boosting techniques like AdaBoost and Gradient Boosting tend to have very high predictive power and often outperform other algorithms.\n",
    "-   **Reduces Bias and Variance**: Boosting reduces both bias (underfitting) and variance (overfitting) in model predictions.\n",
    "-   **Works well with weak learners**: Boosting is particularly useful when base models have high bias (such as shallow trees).\n",
    "\n",
    "#### **Cons:**\n",
    "\n",
    "-   **Computationally Expensive**: Boosting algorithms can be computationally expensive because models are built sequentially.\n",
    "-   **Sensitive to Noisy Data**: Boosting can be sensitive to noisy data or outliers because the algorithm focuses on improving the misclassified instances.\n",
    "-   **Longer Training Time**: Since boosting trains models sequentially, it takes more time to train compared to bagging.\n",
    "\n",
    "----------\n",
    "\n",
    "### **6. Hyperparameters for Boosting Algorithms**\n",
    "\n",
    "Both **AdaBoost** and **Gradient Boosting** have several hyperparameters that you can tune to improve performance:\n",
    "\n",
    "-   **`n_estimators`**: The number of weak learners to train. Increasing this number generally improves performance but also increases computation time.\n",
    "-   **`learning_rate`**: Controls the contribution of each weak learner to the final prediction. Lower values lead to slower but more stable learning.\n",
    "-   **`max_depth`** (for decision trees): Limits the depth of the individual decision trees in the ensemble.\n",
    "\n",
    "----------\n",
    "\n",
    "### **7. Summary of Boosting**\n",
    "\n",
    "-   **AdaBoost** and **Gradient Boosting** are powerful ensemble methods that build models sequentially and focus on improving the performance of weak learners by focusing on hard-to-classify instances.\n",
    "-   **AdaBoost** uses simple decision trees (stumps) and adjusts the weights of misclassified instances. It's faster but might be less flexible than Gradient Boosting.\n",
    "-   **Gradient Boosting** minimizes residual errors using gradient descent and can handle both classification and regression tasks effectively.\n",
    "-   Both methods are prone to overfitting on noisy data but tend to provide excellent performance when tuned correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
